{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zafor158/Vietnam-War-Bombing-Operations-Analysis/blob/main/PySpark_using_RDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1\n",
        "1.Create and Transform an RDD:\n",
        "\n",
        "  **Create** an RDD from a list of integers.\n",
        "* Perform the following:\n",
        "* Multiply each number by 3 using map.\n",
        "* Filter the numbers greater than 10 using filter.\n",
        "* Collect and display the final result."
      ],
      "metadata": {
        "id": "-2LqiD3VAoQE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lDqLY-mYAhPs"
      },
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark= SparkSession.builder.appName(\"RDD_Transformations\").getOrCreate()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=[10,20,30,40,50,60,70,80,90]\n",
        "rdd=spark.sparkContext.parallelize(data)\n",
        "map= rdd.map(lambda x:x*3)\n",
        "filter=map.filter(lambda x:x>10)\n",
        "rdd1_zafor=filter.collect()\n",
        "print(rdd1_zafor)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxgDvb66DT6A",
        "outputId": "4062412b-739b-4ac1-bf0d-49921ccddc1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[30, 60, 90, 120, 150, 180, 210, 240, 270]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task 2\n",
        "2.Read a Text File:\n",
        "* Use textFile to read a file containing sentences (e.g., \"hello world\", \"spark is great\").\n",
        "\n",
        "Perform the following:\n",
        "* Split each line into words using flatMap.\n",
        "* Count the total number of words using count"
      ],
      "metadata": {
        "id": "fjRCLLReKMUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_content = \"\"\"hello world\n",
        "spark is great\n",
        "Python is amazing\n",
        "Data is powerful\n",
        "learning is continuous\"\"\"\n",
        "\n",
        "filename = \"lines.txt\"\n",
        "with open(filename, \"w\") as file:\n",
        "    file.write(file_content)\n",
        "\n",
        "print(f\"File '{filename}' created successfully!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUGeLaygyZgY",
        "outputId": "e227c7c2-9786-49fe-e0f8-511fdc374b5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File 'lines.txt' created successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"ReadTextFile\").getOrCreate()\n",
        "\n",
        "text_file = spark.sparkContext.textFile(\"/content/lines.txt\")\n",
        "\n",
        "words = text_file.flatMap(lambda line: line.split(\" \"))\n",
        "\n",
        "print(words.collect())\n",
        "\n",
        "word_count = words.count()\n",
        "\n",
        "print(\"Total number of words:\", word_count)\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jJ5ymRXxc0H",
        "outputId": "ab6391c0-19c3-4c23-a5ce-43e155743f32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'world', 'spark', 'is', 'great', 'Python', 'is', 'amazing', 'Data', 'is', 'powerful', 'learning', 'is', 'continuous']\n",
            "Total number of words: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 3\n",
        "3.GroupByKey and ReduceByKey:\n",
        "* Create an RDD with pairs of student names and their scores, e.g., [(\"Alice\", 85),\n",
        "(\"Bob\", 90), (\"Alice\", 95)].\n",
        "\n",
        "Use:\n",
        "* groupByKey to group scores by student.\n",
        "* reduceByKey to calculate the total  score for each student."
      ],
      "metadata": {
        "id": "5rroylhxLSyQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pairs=[(\"Alice\", 85), (\"Bob\", 90), (\"Alice\", 95),(\"Bob\",54)]\n",
        "rdd=spark.sparkContext.parallelize(pairs)\n",
        "rdd3_zafor=rdd.groupByKey().mapValues(list)\n",
        "print(rdd3_zafor.collect())\n",
        "rdd3_zafor=rdd.reduceByKey(lambda x,y:x+y)\n",
        "print(rdd3_zafor.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPHsw-EVLm6C",
        "outputId": "74371744-8161-4c89-a6e6-b0d6479cc011"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Alice', [85, 95]), ('Bob', [90, 54])]\n",
            "[('Alice', 180), ('Bob', 144)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 4\n",
        "RDD Persistence:\n",
        "* Create an RDD from a large list (simulate it by generating numbers from 1 to 1,000,000).\n",
        "* Perform multiple actions (e.g., count, sum) without caching and measure execution\n",
        "time.\n",
        "* Repeat with cache or persist and compare the performance."
      ],
      "metadata": {
        "id": "adL1ccmYNNUB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "# Without caching\n",
        "start_time = time.time()\n",
        "data = list(range(1, 1000001))\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "count = rdd.count()\n",
        "sum_val = rdd.sum()\n",
        "end_time = time.time()\n",
        "print(f\"Without caching - Count: {count}, Sum: {sum_val}\")\n",
        "print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# With caching\n",
        "start_time = time.time()\n",
        "rdd.cache()  # or rdd.persist()\n",
        "count = rdd.count()\n",
        "sum_val = rdd.sum()\n",
        "end_time = time.time()\n",
        "print(f\"With caching - Count: {count}, Sum: {sum_val}\")\n",
        "print(f\"Time taken: {end_time - start_time:.4f} seconds\")\n",
        "\n",
        "# Further actions (demonstrating the benefit of caching)\n",
        "start_time = time.time()\n",
        "count = rdd.count()\n",
        "sum_val = rdd.sum()\n",
        "end_time = time.time()\n",
        "print(f\"With caching (repeated actions) - Count: {count}, Sum: {sum_val}\")\n",
        "print(f\"Time taken: {end_time - start_time:.4f} seconds\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_435iC9jOhjp",
        "outputId": "bc3b7710-6897-43f2-a4f2-b60568ea7cf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without caching - Count: 1000000, Sum: 500000500000\n",
            "Time taken: 1.7239 seconds\n",
            "With caching - Count: 1000000, Sum: 500000500000\n",
            "Time taken: 1.2471 seconds\n",
            "With caching (repeated actions) - Count: 1000000, Sum: 500000500000\n",
            "Time taken: 1.0951 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 5\n",
        "Custom Transformations:\n",
        "* Create an RDD of numbers.\n",
        "* Write a custom transformation to identify and filter out prime numbers."
      ],
      "metadata": {
        "id": "K5R8Cb2SRZ84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_prime(n):\n",
        "    if n <= 1:\n",
        "        return False\n",
        "    for i in range(2, int(n**0.5) + 1):\n",
        "        if n % i == 0:\n",
        "            return False\n",
        "    return True\n",
        "\n",
        "numbers = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
        "rdd = spark.sparkContext.parallelize(numbers)\n",
        "\n",
        "prime_rdd = rdd.filter(is_prime)\n",
        "\n",
        "prime_numbers = prime_rdd.collect()\n",
        "print(f\"Prime numbers: {prime_numbers}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ii35O60ERhqo",
        "outputId": "9554bf95-85c4-457e-e7e7-93896bb009fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prime numbers: [2, 3, 5, 7, 11, 13]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 6\n",
        "Transformation and Action Workflow:\n",
        "* Load a dataset (e.g., CSV or text file) containing the following:\n",
        "```\n",
        "Product,Category,Price\n",
        "Laptop,Electronics,800\n",
        "Shoes,Clothing,50\n",
        "Phone,Electronics,500\n",
        "```\n",
        "Perform the following:\n",
        "* Filter products with a price greater than 100.\n",
        "* Map to get only the product names.\n",
        "* Count the number of products in each category using map and reduceByKey."
      ],
      "metadata": {
        "id": "g8Zz9s1xSxId"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"RDD_Transformations\").getOrCreate()\n",
        "data = [\n",
        "    (\"Laptop\", \"Electronics\", 800),\n",
        "    (\"Shoes\", \"Clothing\", 50),\n",
        "    (\"Phone\", \"Electronics\", 500),\n",
        "    (\"Tablet\", \"Electronics\", 300),\n",
        "    (\"Shirt\", \"Clothing\", 25),\n",
        "    (\"Headphones\", \"Electronics\", 100),\n",
        "    (\"Watch\", \"Accessories\", 150),\n",
        "    (\"Sunglasses\", \"Accessories\", 80),\n",
        "    (\"Jacket\", \"Clothing\", 120),\n",
        "    (\"Backpack\", \"Accessories\", 60),\n",
        "    (\"Television\", \"Electronics\", 1000),\n",
        "    (\"Microwave\", \"Home Appliances\", 200),\n",
        "    (\"Refrigerator\", \"Home Appliances\", 1200),\n",
        "    (\"Jeans\", \"Clothing\", 40),\n",
        "    (\"Sweater\", \"Clothing\", 35),\n",
        "    (\"Blender\", \"Home Appliances\", 70),\n",
        "    (\"Gaming Console\", \"Electronics\", 400),\n",
        "    (\"Desk Lamp\", \"Furniture\", 45),\n",
        "    (\"Office Chair\", \"Furniture\", 150),\n",
        "    (\"Table\", \"Furniture\", 300)\n",
        "]\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(data)\n",
        "#rdd.saveAsTextFile(\"data1.txt\")\n",
        "#new_rdd=spark.sparkContext.textFile(\"/content/data1.txt\")# read textfile data\n",
        "# Filter products with a price greater than 100\n",
        "filtered_rdd = rdd.filter(lambda x: x[2] > 100)\n",
        "\n",
        "# Map to get only the product names and categories\n",
        "product_category_rdd = filtered_rdd.map(lambda x: (x[0], x[1]))\n",
        "\n",
        "# Count the number of products in each category\n",
        "product_counts = product_category_rdd.map(lambda x: (x[1], 1)).reduceByKey(lambda x, y: x + y)\n",
        "\n",
        "#Collect and print the results\n",
        "print(f\"Products with price greater than 100: {filtered_rdd.collect()}\\n\")\n",
        "print(f\"Product and categories: {product_category_rdd.collect()}\\n\")\n",
        "print(\"Number of products in each category:\")\n",
        "for category, count in product_counts.collect():\n",
        "    print(f\"{category}: {count}\")\n",
        "\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vfePbOxPTInG",
        "outputId": "94f3f150-a32d-4e7a-89ce-3ff3a0ae7734"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with price greater than 100: [('Laptop', 'Electronics', 800), ('Phone', 'Electronics', 500), ('Tablet', 'Electronics', 300), ('Watch', 'Accessories', 150), ('Jacket', 'Clothing', 120), ('Television', 'Electronics', 1000), ('Microwave', 'Home Appliances', 200), ('Refrigerator', 'Home Appliances', 1200), ('Gaming Console', 'Electronics', 400), ('Office Chair', 'Furniture', 150), ('Table', 'Furniture', 300)]\n",
            "\n",
            "Product and categories: [('Laptop', 'Electronics'), ('Phone', 'Electronics'), ('Tablet', 'Electronics'), ('Watch', 'Accessories'), ('Jacket', 'Clothing'), ('Television', 'Electronics'), ('Microwave', 'Home Appliances'), ('Refrigerator', 'Home Appliances'), ('Gaming Console', 'Electronics'), ('Office Chair', 'Furniture'), ('Table', 'Furniture')]\n",
            "\n",
            "Number of products in each category:\n",
            "Electronics: 5\n",
            "Accessories: 1\n",
            "Furniture: 2\n",
            "Clothing: 1\n",
            "Home Appliances: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 7\n",
        "Integration with Spark SQL:\n",
        "\n",
        "* Load a JSON dataset containing information about students:\n",
        "```\n",
        "[{\"name\": \"Alice\", \"age\": 20, \"grade\": \"A\"},\n",
        "{\"name\": \"Bob\", \"age\": 22, \"grade\": \"B\"}]\n",
        "```\n",
        "\n",
        "\n",
        "Perform the following:\n",
        "* Register the data as a temporary SQL table.\n",
        "* Query students who have a grade \"A\" using Spark SQL.\n",
        "* Save the result to a new JSON file.\n"
      ],
      "metadata": {
        "id": "24UsBRjVv1Ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder.appName(\"SparkSQLIntegration\").getOrCreate()\n",
        "\n",
        "# JSON dataset\n",
        "data = '''[\n",
        "    {\"name\": \"Alice\", \"age\": 20, \"grade\": \"A\"},\n",
        "    {\"name\": \"Bob\", \"age\": 22, \"grade\": \"B\"},\n",
        "    {\"name\": \"Moe\", \"age\": 21, \"grade\": \"A\"},\n",
        "    {\"name\": \"Jane\", \"age\": 19, \"grade\": \"C\"},\n",
        "    {\"name\": \"Tom\", \"age\": 23, \"grade\": \"B\"},\n",
        "    {\"name\": \"Sara\", \"age\": 20, \"grade\": \"A\"},\n",
        "    {\"name\": \"Liam\", \"age\": 24, \"grade\": \"D\"},\n",
        "    {\"name\": \"Emma\", \"age\": 22, \"grade\": \"B\"},\n",
        "    {\"name\": \"Noah\", \"age\": 21, \"grade\": \"A\"},\n",
        "    {\"name\": \"Olivia\", \"age\": 20, \"grade\": \"C\"}\n",
        "]'''\n",
        "\n",
        "json_file = \"students.json\"\n",
        "\n",
        "# Save the data to a JSON file\n",
        "with open(json_file, \"w\") as f:\n",
        "    f.write(data)\n",
        "\n",
        "# Load the JSON file into a Spark DataFrame\n",
        "df = spark.read.json(json_file)\n",
        "\n",
        "# Register the DataFrame as a temporary SQL table\n",
        "df.createOrReplaceTempView(\"students\")\n",
        "\n",
        "# Query students with grade \"A\" using Spark SQL\n",
        "result_df = spark.sql(\"SELECT * FROM students WHERE grade = 'A'\")\n",
        "\n",
        "# Save the result to a new JSON file\n",
        "output_file = \"filtered_students.json\"\n",
        "# Save the result to a new JSON file with overwrite mode\n",
        "result_df.write.mode(\"overwrite\").json(output_file)\n",
        "\n",
        "\n",
        "# Display the result\n",
        "result_df.show()\n",
        "\n",
        "# Stop the Spark session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9CyHgwfHw25O",
        "outputId": "35a79ed2-bbe9-415c-dac9-45b297ff88bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+---+-----+-----+\n",
            "|_corrupt_record|age|grade| name|\n",
            "+---------------+---+-----+-----+\n",
            "|           NULL| 20|    A|Alice|\n",
            "|           NULL| 21|    A|  Moe|\n",
            "|           NULL| 20|    A| Sara|\n",
            "|           NULL| 21|    A| Noah|\n",
            "+---------------+---+-----+-----+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 8\n",
        "Advanced Word Count with Sorting:\n",
        "\n",
        "Extend the word count program to:\n",
        "* sort words by their frequency in descending order.\n",
        "* Display the top 5 most frequent words."
      ],
      "metadata": {
        "id": "6xmRTOJ80KyX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder.appName(\"AdvancedWordCount\").getOrCreate()\n",
        "\n",
        "# Sample text data\n",
        "text_data = [\"hello world hello\", \"spark is great\", \"hello spark\", \"world is big\", \"hello world\"]\n",
        "rdd = spark.sparkContext.parallelize(text_data)\n",
        "\n",
        "# Split into words and count frequencies\n",
        "word_counts = rdd.flatMap(lambda line: line.split()) \\\n",
        "    .map(lambda word: (word, 1)) \\\n",
        "    .reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "\n",
        "# Sort by frequency in descending order\n",
        "sorted_word_counts = word_counts.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Get the top 5 most frequent words\n",
        "top_5_words = sorted_word_counts.take(5)\n",
        "\n",
        "# Display the results\n",
        "print(\"Top 5 most frequent words:\")\n",
        "for word, count in top_5_words:\n",
        "    print(f\"{word}: {count}\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "yiVQE5EG0VvH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d65e9060-de02-472d-fa05-48033202ce82"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 most frequent words:\n",
            "hello: 4\n",
            "world: 3\n",
            "is: 2\n",
            "spark: 2\n",
            "great: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 9\n",
        "Custom Aggregations with aggregateByKey:\n",
        "* Create an RDD with pairs of cities and temperatures, e.g., [(\"NY\", 32), (\"LA\",\n",
        "75), (\"NY\", 28)].\n",
        "\n",
        "* Use aggregateByKey to calculate the average temperature for each city"
      ],
      "metadata": {
        "id": "mqYyZSGo09Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CustomAggregations\").getOrCreate()\n",
        "\n",
        "# Create an RDD with city-temperature pairs\n",
        "city_temps = [(\"NY\", 32), (\"LA\", 75), (\"NY\", 28), (\"LA\", 80), (\"Chicago\", 45), (\"NY\", 35), (\"Chicago\", 50), (\"LA\", 70)]\n",
        "rdd = spark.sparkContext.parallelize(city_temps)\n",
        "\n",
        "# Use aggregateByKey to calculate the average temperature for each city\n",
        "def seq_op(acc, temp):\n",
        "    # Accumulator: (sum of temperatures, count of temperatures)\n",
        "    return (acc[0] + temp, acc[1] + 1)\n",
        "\n",
        "def comb_op(acc1, acc2):\n",
        "    return (acc1[0] + acc2[0], acc1[1] + acc2[1])\n",
        "\n",
        "zero_value = (0, 0) # Initial accumulator value\n",
        "\n",
        "avg_temps = rdd.aggregateByKey(zero_value, seq_op, comb_op) \\\n",
        "    .mapValues(lambda x: x[0] / x[1])\n",
        "\n",
        "# Collect and print the results\n",
        "for city, avg_temp in avg_temps.collect():\n",
        "    print(f\"Average temperature in {city}: {avg_temp}\")\n",
        "\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "2MqkNmkH1M9X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6430985d-dcb9-4101-edba-c4097f42cedc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average temperature in NY: 31.666666666666668\n",
            "Average temperature in LA: 75.0\n",
            "Average temperature in Chicago: 47.5\n"
          ]
        }
      ]
    }
  ]
}